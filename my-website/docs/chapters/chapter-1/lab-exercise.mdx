---
title: "Lab Exercise: Exploring Physical AI Concepts"
sidebar_position: 9
---

# Lab Exercise: Exploring Physical AI Concepts

## Objective

This lab exercise is designed to provide hands-on experience with fundamental Physical AI concepts through practical implementation. Students will implement a simple Physical AI system that demonstrates the perception-action loop, sensor integration, and basic control algorithms. The exercise will help solidify understanding of how AI algorithms interact with physical systems and the challenges inherent in real-world AI applications.

By completing this lab, students will:
- Implement a basic Physical AI system using ROS 2
- Integrate multiple sensor modalities for environmental perception
- Design and implement a simple control system for physical interaction
- Understand the challenges of real-time processing in physical systems
- Experience the iterative process of developing Physical AI systems

## Prerequisites

Before starting this lab exercise, students should have:
- Basic knowledge of Python programming
- Understanding of fundamental robotics concepts (kinematics, control)
- Familiarity with ROS 2 concepts (nodes, topics, messages)
- Access to a computer capable of running ROS 2 (Ubuntu 20.04/22.04 or equivalent)
- Basic understanding of linear algebra and calculus for kinematics

## Setup Instructions

### Software Requirements
1. Install ROS 2 Humble Hawksbill (or latest LTS version)
2. Install Python 3.8 or higher
3. Install required Python packages:
   ```bash
   pip install numpy matplotlib opencv-python transforms3d
   ```
4. Set up a ROS 2 workspace:
   ```bash
   mkdir -p ~/physical_ai_ws/src
   cd ~/physical_ai_ws
   colcon build
   source install/setup.bash
   ```

### Simulation Environment (Optional)
For those without physical robots, you can use Gazebo simulation:
1. Install Gazebo Garden: `sudo apt install ros-humble-gazebo-ros-pkgs`
2. Install ROS 2 Gazebo plugins
3. Set up a simple differential drive robot model

## Exercise 1: Perception System Implementation

### Task Description
Implement a perception system that integrates data from multiple sensors to understand the environment. This system will serve as the foundation for the Physical AI system.

### Implementation Steps

1. **Create a ROS 2 node for perception:**
   ```python
   import rclpy
   from rclpy.node import Node
   from sensor_msgs.msg import LaserScan, Image
   from geometry_msgs.msg import PointStamped
   from cv_bridge import CvBridge
   import numpy as np
   import cv2

   class PerceptionNode(Node):
       def __init__(self):
           super().__init__('perception_node')

           # Initialize CV bridge for image processing
           self.cv_bridge = CvBridge()

           # Create subscribers for sensors
           self.laser_sub = self.create_subscription(
               LaserScan,
               '/scan',
               self.laser_callback,
               10
           )

           self.camera_sub = self.create_subscription(
               Image,
               '/camera/image_raw',
               self.camera_callback,
               10
           )

           # Publisher for detected objects
           self.object_pub = self.create_publisher(
               PointStamped,
               '/detected_objects',
               10
           )

           # Store sensor data
           self.latest_laser_data = None
           self.latest_image = None

           # Timer for processing loop
           self.process_timer = self.create_timer(0.1, self.process_sensors)

       def laser_callback(self, msg):
           self.latest_laser_data = msg

       def camera_callback(self, msg):
           try:
               self.latest_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")
           except Exception as e:
               self.get_logger().error(f'Error converting image: {e}')

       def process_sensors(self):
           if self.latest_laser_data and self.latest_image:
               # Detect objects using both sensors
               laser_objects = self.detect_objects_laser(self.latest_laser_data)
               vision_objects = self.detect_objects_vision(self.latest_image)

               # Fuse sensor data (simplified)
               fused_objects = self.fuse_sensor_data(laser_objects, vision_objects)

               # Publish results
               for obj in fused_objects:
                   point_msg = PointStamped()
                   point_msg.point.x = obj['x']
                   point_msg.point.y = obj['y']
                   point_msg.point.z = obj['z']
                   self.object_pub.publish(point_msg)

       def detect_objects_laser(self, laser_msg):
           """Detect objects using laser scan data"""
           objects = []
           for i, range_val in enumerate(laser_msg.ranges):
               if range_val < 1.0 and range_val > 0.1:  # Within 1m
                   angle = laser_msg.angle_min + i * laser_msg.angle_increment
                   x = range_val * np.cos(angle)
                   y = range_val * np.sin(angle)
                   objects.append({'x': x, 'y': y, 'type': 'obstacle'})
           return objects

       def detect_objects_vision(self, image):
           """Detect objects using computer vision"""
           # Convert to HSV for color detection
           hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

           # Define color ranges for objects (e.g., red objects)
           lower_red = np.array([0, 50, 50])
           upper_red = np.array([10, 255, 255])

           mask = cv2.inRange(hsv, lower_red, upper_red)
           contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

           objects = []
           for contour in contours:
               if cv2.contourArea(contour) > 500:  # Filter small contours
                   M = cv2.moments(contour)
                   if M["m00"] != 0:
                       cX = int(M["m10"] / M["m00"])
                       cY = int(M["m01"] / M["m00"])
                       objects.append({'x': cX, 'y': cY, 'type': 'red_object'})

           return objects

       def fuse_sensor_data(self, laser_objects, vision_objects):
           """Simple sensor fusion"""
           # For this exercise, just return laser objects
           # In practice, you would implement more sophisticated fusion
           return laser_objects
   ```

2. **Test the perception system:**
   - Run your ROS 2 node: `ros2 run your_package perception_node`
   - Verify that the system publishes detected objects
   - Use `rviz2` to visualize the detected objects

## Exercise 2: Control System Implementation

### Task Description
Implement a simple control system that uses the perception data to navigate toward a target while avoiding obstacles.

### Implementation Steps

1. **Create a control node:**
   ```python
   import rclpy
   from rclpy.node import Node
   from geometry_msgs.msg import Twist, PointStamped
   from sensor_msgs.msg import LaserScan
   import numpy as np

   class ControlNode(Node):
       def __init__(self):
           super().__init__('control_node')

           # Publisher for robot velocity commands
           self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)

           # Subscriber for detected objects
           self.object_sub = self.create_subscription(
               PointStamped,
               '/detected_objects',
               self.object_callback,
               10
           )

           # Subscriber for laser scan (for obstacle avoidance)
           self.laser_sub = self.create_subscription(
               LaserScan,
               '/scan',
               self.laser_callback,
               10
           )

           # Target position (we'll navigate toward this)
           self.target_x = 5.0
           self.target_y = 0.0

           # Robot position (will be updated from odometry in real implementation)
           self.robot_x = 0.0
           self.robot_y = 0.0

           # Control parameters
           self.linear_speed = 0.3
           self.angular_speed = 0.5

           # Timer for control loop
           self.control_timer = self.create_timer(0.05, self.control_loop)

           # Store sensor data
           self.detected_objects = []
           self.laser_ranges = []

       def object_callback(self, msg):
           self.detected_objects.append({
               'x': msg.point.x,
               'y': msg.point.y
           })
           # Keep only recent detections
           if len(self.detected_objects) > 10:
               self.detected_objects = self.detected_objects[-10:]

       def laser_callback(self, msg):
           self.laser_ranges = list(msg.ranges)

       def control_loop(self):
           cmd_vel = Twist()

           # Calculate direction to target
           target_dx = self.target_x - self.robot_x
           target_dy = self.target_y - self.robot_y
           target_distance = np.sqrt(target_dx**2 + target_dy**2)

           # Calculate target angle
           target_angle = np.arctan2(target_dy, target_dx)

           # Simple obstacle avoidance
           obstacle_detected = self.check_for_obstacles()

           if obstacle_detected:
               # Implement simple obstacle avoidance
               cmd_vel.linear.x = 0.1  # Slow down
               cmd_vel.angular.z = self.angular_speed  # Turn to avoid
           elif target_distance > 0.5:  # If not close to target
               # Move toward target
               cmd_vel.linear.x = self.linear_speed
               cmd_vel.angular.z = target_angle * 0.5  # Simple proportional control
           else:
               # Reached target
               cmd_vel.linear.x = 0.0
               cmd_vel.angular.z = 0.0

           # Publish command
           self.cmd_vel_pub.publish(cmd_vel)

       def check_for_obstacles(self):
           """Check laser scan for obstacles in front of robot"""
           if not self.laser_ranges:
               return False

           # Check the front 30 degrees of the laser scan
           front_ranges = self.laser_ranges[:len(self.laser_ranges)//12] + \
                         self.laser_ranges[-len(self.laser_ranges)//12:]

           # Filter out invalid readings
           valid_ranges = [r for r in front_ranges if r > 0 and r < 1.0]

           return len(valid_ranges) > 0
   ```

2. **Test the control system:**
   - Combine with the perception system
   - Use simulation or a physical robot to test navigation
   - Adjust control parameters for better performance

## Exercise 3: Integration and Testing

### Task Description
Integrate the perception and control systems, then test the complete Physical AI system in various scenarios.

### Implementation Steps

1. **Create a main function that runs both nodes:**
   ```python
   def main(args=None):
       rclpy.init(args=args)

       # Create both nodes
       perception_node = PerceptionNode()
       control_node = ControlNode()

       # Create executor and add nodes
       executor = rclpy.executors.MultiThreadedExecutor()
       executor.add_node(perception_node)
       executor.add_node(control_node)

       try:
           executor.spin()
       except KeyboardInterrupt:
           pass
       finally:
           perception_node.destroy_node()
           control_node.destroy_node()
           rclpy.shutdown()

   if __name__ == '__main__':
       main()
   ```

2. **Testing scenarios:**
   - **Open space navigation:** Test navigation to target in an obstacle-free environment
   - **Obstacle avoidance:** Place obstacles between robot and target
   - **Moving target:** Implement a moving target that changes position
   - **Sensor noise:** Add artificial noise to sensor readings to test robustness

## Analysis and Discussion

### Questions to Consider
1. How does sensor noise affect the performance of your Physical AI system?
2. What are the limitations of the simple control algorithms used in this exercise?
3. How would you improve the sensor fusion in a real-world application?
4. What safety considerations are important when implementing Physical AI systems?

### Performance Metrics
- **Navigation success rate:** Percentage of successful navigations to target
- **Time to target:** Average time to reach target position
- **Path efficiency:** Ratio of direct distance to actual path length
- **Obstacle avoidance success:** Percentage of successful obstacle avoidance maneuvers

## Advanced Extensions (Optional)

1. **Implement more sophisticated perception:**
   - Use deep learning for object detection
   - Implement SLAM for mapping and localization
   - Add tactile sensing for physical interaction

2. **Advanced control strategies:**
   - Implement model predictive control (MPC)
   - Use reinforcement learning for navigation
   - Add formation control for multiple robots

3. **Real-world deployment:**
   - Test on a physical robot platform
   - Integrate with more complex simulation environments
   - Add communication with other Physical AI systems

## Key Takeaways

- Physical AI systems require tight integration of perception, reasoning, and action
- Sensor fusion is crucial for robust environmental understanding
- Real-time processing constraints are fundamental in Physical AI
- Control algorithms must be robust to sensor noise and environmental uncertainties
- Testing and validation are critical for safe Physical AI deployment
- The iterative process of implementing, testing, and refining is essential for Physical AI development